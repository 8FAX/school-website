<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AI in Data Validation - Midterm project by Liam Scott</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css"
    />
    <link rel="stylesheet" href="styles.css">
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/themes/prism.min.css"
    />
</head>
<body data-spy="scroll" data-target="#navbar" data-offset="70">
    <nav id="navbar" class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
      <a class="navbar-brand" href="#">AI in Data Validation - Midterm project by Liam Scott</a>
      <button
        class="navbar-toggler"
        type="button"
        data-toggle="collapse"
        data-target="#navbarNav"
      >
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item"><a class="nav-link" href="#introduction">Introduction</a></li>
          <li class="nav-item"><a class="nav-link" href="#applications">AI Applications</a></li>
          <li class="nav-item"><a class="nav-link" href="#tools">Tools & Examples</a></li>
          <li class="nav-item"><a class="nav-link" href="#integration">Integration</a></li>
          <li class="nav-item"><a class="nav-link" href="#pros-cons">Pros & Cons</a></li>
          <li class="nav-item"><a class="nav-link" href="#additional-considerations">Considerations</a></li>
          <li class="nav-item"><a class="nav-link" href="#future">Future</a></li>
          <li class="nav-item"><a class="nav-link" href="#conclusion">Conclusion</a></li>
          <li class="nav-item"><a class="nav-link" href="#resources">Resources</a></li>
        </ul>
      </div>
    </nav>

    <header class="jumbotron jumbotron-fluid text-center text-white animate__animated animate__fadeIn">
        <div class="container">
            <h1 class="display-4">Use of AI in Data Validation</h1>
            <p class="lead">With Examples and Code</p>
            <p class="lead">Midterm project by Liam Scott</p>
        </div>
    </header>

    <main class="container">
      <section id="introduction" class="animate__animated animate__fadeInUp">
        <h2>Introduction</h2>
        <p>
            Data validation is a cornerstone of data management, particularly in platforms that rely heavily on user-generated content, where ensuring data accuracy and consistency is critical. As Chief Technology Officers (CTOs) and data leaders explore new ways to scale and optimize data workflows, artificial intelligence (AI) technologies offer powerful methods to automate and enhance data validation processes. This guide is designed for both decision-makers, like CTOs looking to integrate AI into their data management strategies, and for developers or data analysts seeking practical AI tools and solutions to improve data quality and efficiency.
        </p>
        <p>
            In this guide, we’ll explore how AI can revolutionize data validation by covering key use cases such as anomaly detection, text correction, data matching, and more. You'll find detailed examples of how specific AI tools—like OpenAI's GPT-4, spaCy, and TensorFlow—can be implemented in real-world scenarios. Each section includes practical code snippets to help developers and analysts integrate these AI solutions directly into their data pipelines.
        </p>
    
        <h3>What This Guide Covers:</h3>
        <ul>
            <li><strong>AI Applications in Data Validation:</strong> How AI can correct, standardize, and validate data across various tasks.</li>
            <li><strong>Specific Tools and Code Examples:</strong> Hands-on demonstrations using popular AI libraries to automate validation tasks.</li>
            <li><strong>Integrating AI into Data Pipelines:</strong> Best practices for real-time and batch processing, error handling, and model evaluation.</li>
            <li><strong>Pros and Cons of AI in Data Validation:</strong> A balanced look at the benefits and challenges of using AI-driven tools.</li>
            <li><strong>Future Technological Trends:</strong> An exploration of upcoming AI innovations, including edge AI and ethical considerations.</li>
        </ul>
        <p>
            Whether you're looking to transform your organization’s data management approach or you're a developer ready to implement AI-driven validation techniques, this guide will provide the knowledge and tools needed to succeed.
        </p>
    </section>
    
    
        <hr>

        <section id="applications" class="animate__animated animate__fadeInUp">
            <h2>AI Applications in Data Validation</h2>
            <h3>1. Text Correction and Standardization</h3>
            <ul>
                <li><strong>Spell Checking and Grammar Correction</strong>: AI models can automatically correct misspellings and grammatical errors in user inputs.</li>
                <li><strong>Normalization</strong>: Standardizing data formats (e.g., dates, phone numbers) to ensure consistency.</li>
            </ul>

            <h3>2. Anomaly and Outlier Detection</h3>
            <ul>
                <li><strong>Statistical Methods</strong>: AI can identify data points that deviate significantly from the norm.</li>
                <li><strong>Machine Learning Models</strong>: Algorithms like Isolation Forest or Autoencoders detect anomalies in complex datasets.</li>
            </ul>

            <h3>3. Entity Recognition and Extraction</h3>
            <ul>
                <li><strong>Named Entity Recognition (NER)</strong>: Extract structured information such as names, addresses, and product details from unstructured text.</li>
                <li><strong>Classification</strong>: Categorize data entries into predefined classes.</li>
            </ul>

            <h3>4. Data Matching and Deduplication</h3>
            <ul>
                <li><strong>Fuzzy Matching</strong>: Identify duplicate records that are not exact matches but represent the same entity.</li>
                <li><strong>Semantic Similarity</strong>: Use AI to compare the meaning of text entries.</li>
            </ul>

            <h3>5. Automated Data Formatting</h3>
            <ul>
                <li><strong>Schema Enforcement</strong>: Ensure that data entries conform to a predefined schema.</li>
                <li><strong>Data Transformation</strong>: Convert data from one format to another using AI-driven rules.</li>
            </ul>
        </section>

        <hr>

        <section id="tools" class="animate__animated animate__fadeInUp">
            <h2>Specific Tools and Implementation with Code Examples</h2>

            <h3>1. OpenAI's GPT-4 API</h3>
            <p>GPT-4 can be used for text correction, entity extraction, and data formatting.</p>

            <h4>Example: Correcting and Formatting User Input</h4>
            <pre><code class="language-python">
import openai
import json

openai.api_key = 'YOUR_OPENAI_API_KEY'

def correct_and_format(text):
    prompt = f"""
    You are a data validation assistant.
    Correct the following text for spelling and grammar errors.
    Then, extract the following information:
    - Name
    - Address
    - Email
    - Phone Number

    Provide the output in JSON format.

    Text: "{text}"
    """

    response = openai.ChatCompletion.create(
        model='gpt-4',
        messages=[{'role': 'user', 'content': prompt}],
        max_tokens=150,
        temperature=0
    )

    return response['choices'][0]['message']['content']

user_input = "helo, my nme is Alex Jonson. I live at 742 Evergren Terrce, Sprngfeld. Contact me at alex.jonsn@@email.com or 555-2368."
result = correct_and_format(user_input)
print(result)
            </code></pre>

            <p><strong>Expected Output:</strong></p>
            <pre><code class="language-json">
{
  "Name": "Alex Johnson",
  "Address": "742 Evergreen Terrace, Springfield",
  "Email": "alex.johnson@email.com",
  "Phone Number": "555-2368"
}
            </code></pre>

            <h3>2. spaCy for Natural Language Processing</h3>
            <p>spaCy is an efficient library for NLP tasks, including NER.</p>

            <h4>Example: Extracting Entities from Text</h4>
            <pre><code class="language-python">
import spacy

# Load the pre-trained model
nlp = spacy.load('en_core_web_sm')

def extract_entities(text):
    doc = nlp(text)
    entities = {}
    for ent in doc.ents:
        entities.setdefault(ent.label_, []).append(ent.text)
    return entities

text = "Order 12345 was placed on 2023-10-17 by John Doe from 1600 Amphitheatre Parkway, Mountain View, CA."
entities = extract_entities(text)
print(entities)
            </code></pre>

            <p><strong>Expected Output:</strong></p>
            <pre><code class="language-json">
{
  "DATE": ["2023-10-17"],
  "PERSON": ["John Doe"],
  "CARDINAL": ["12345"],
  "ORG": ["Amphitheatre Parkway"],
  "GPE": ["Mountain View", "CA"]
}
            </code></pre>

            <h3>3. TensorFlow for Machine Learning Models</h3>
            <p>TensorFlow can be used to build custom models for anomaly detection.</p>

            <h4>Example: Building an Autoencoder for Anomaly Detection</h4>
            <pre><code class="language-python">
import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np

# Generate synthetic data
data = np.random.normal(0, 1, (1000, 20))

# Define the autoencoder model
input_dim = data.shape[1]
encoding_dim = 10

input_layer = layers.Input(shape=(input_dim,))
encoded = layers.Dense(encoding_dim, activation='relu')(input_layer)
decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = models.Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train the model
autoencoder.fit(data, data, epochs=50, batch_size=32, shuffle=True)

# Function to detect anomalies
def detect_anomaly(sample):
    reconstruction = autoencoder.predict(sample.reshape(1, -1))
    mse = np.mean(np.power(sample - reconstruction, 2))
    return mse

# Test with a new sample
test_sample = np.random.normal(0, 1, (20,))
anomaly_score = detect_anomaly(test_sample)
print(f'Anomaly Score: {anomaly_score}')
            </code></pre>

            <h3>4. LangChain for Building Applications with LLMs</h3>
            <p>LangChain simplifies building applications powered by LLMs with features like prompt templates and retry logic.</p>

            <h4>Example: Data Formatting with Retry Logic</h4>
            <pre><code class="language-python">
from langchain.llms import OpenAI
from langchain import PromptTemplate, LLMChain
import json

llm = OpenAI(openai_api_key='YOUR_OPENAI_API_KEY')

template = PromptTemplate(
    input_variables=['data'],
    template="""
    Format the following data into JSON with keys 'product_name', 'price', and 'description'. Ensure all fields are present.

    Data: "{data}"

    Output only the JSON.

    If any information is missing, respond with 'Incomplete data'.
    """
)

chain = LLMChain(llm=llm, prompt=template)

def format_data_with_retry(data, max_attempts=3):
    for attempt in range(max_attempts):
        response = chain.run(data=data)
        if 'Incomplete data' not in response:
            try:
                json_data = json.loads(response)
                return json_data
            except json.JSONDecodeError:
                continue
    return None

raw_data = "Product: SuperWidget, Price: $19.99, Description: A useful widget."
formatted_data = format_data_with_retry(raw_data)
print(formatted_data)
            </code></pre>

            <p><strong>Expected Output:</strong></p>
            <pre><code class="language-json">
{
  "product_name": "SuperWidget",
  "price": 19.99,
  "description": "A useful widget."
}
            </code></pre>

            <h3>5. Playwright for Web Scraping and Validation</h3>
            <p>Playwright automates browser interactions, enabling data scraping and validation.</p>

            <h4>Example: Scraping Data and Validating with AI</h4>
            <pre><code class="language-python">
import asyncio
from playwright.async_api import async_playwright
import openai

openai.api_key = 'YOUR_OPENAI_API_KEY'

async def scrape_and_validate(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url)

        # Scrape data
        content = await page.content()

        # Validate data using GPT-4
        prompt = f"Validate the following HTML content and extract product information in JSON format:\n\n{content}"
        response = openai.ChatCompletion.create(
            model='gpt-4',
            messages=[{'role': 'user', 'content': prompt}],
            max_tokens=500,
            temperature=0
        )
        await browser.close()
        return response['choices'][0]['message']['content']

# URL of the page to scrape
url = 'https://example.com/product-page'

# Run the scraping and validation
result = asyncio.run(scrape_and_validate(url))
print(result)
            </code></pre>
        </section>

        <hr>

        <section id="integration" class="animate__animated animate__fadeInUp">
            <h2>Integrating AI Tools into Data Pipelines</h2>

            <h3>1. Real-Time Data Validation</h3>
            <ul>
                <li>Incorporate AI validation in the data ingestion pipeline to process data as it arrives.</li>
                <li>Use asynchronous processing to handle high data throughput without bottlenecks.</li>
            </ul>

            <h3>2. Batch Processing</h3>
            <ul>
                <li>Schedule regular batch jobs to clean and validate existing datasets.</li>
                <li>Utilize distributed computing frameworks like Apache Spark for scalability.</li>
            </ul>

            <h3>3. Retry Logic and Error Handling</h3>
            <ul>
                <li>Implement retry mechanisms when AI services fail or return incomplete results.</li>
                <li>Log errors and fallback to default validation methods if necessary.</li>
            </ul>

            <h3>4. Monitoring and Evaluation</h3>
            <ul>
                <li>Track the performance of AI models in production.</li>
                <li>Use evaluation metrics to assess accuracy and adjust models accordingly.</li>
            </ul>
        </section>

        <hr>

        <section id="pros-cons" class="animate__animated animate__fadeInUp">
            <h2>Pros and Cons of Using AI Tools</h2>

            <h3>Pros:</h3>
            <ul>
                <li><strong>Scalability</strong>: Handle large volumes of data efficiently.</li>
                <li><strong>Accuracy</strong>: Improve data quality with advanced validation techniques.</li>
                <li><strong>Automation</strong>: Reduce manual intervention, saving time and resources.</li>
                <li><strong>Adaptability</strong>: AI models can be updated and retrained with new data patterns.</li>
            </ul>

            <h3>Cons:</h3>
            <ul>
                <li><strong>Cost</strong>: API usage and computational resources can be expensive.</li>
                <li><strong>Complexity</strong>: Requires technical expertise to implement and maintain.</li>
                <li><strong>Latency</strong>: Real-time validation may introduce delays.</li>
                <li><strong>Bias and Errors</strong>: AI models can inherit biases from training data.</li>
            </ul>
        </section>

        
        <hr>
        <section id="additional-considerations" class="animate__animated animate__fadeInUp">
          <h2>Additional Considerations</h2>
          <p>
              In addition to the opportunities provided by AI in data validation, there are several challenges and considerations that require attention to ensure successful implementation. These include mitigating potential drawbacks such as cost, complexity, and bias, as well as selecting the right tools for specific tasks and addressing ethical concerns.
          </p>
      
          <h3>1. Challenges and Mitigation Strategies</h3>
          <p>While AI offers significant advantages in data validation, several challenges must be addressed for it to be truly effective:</p>
      
          <h4>Cost</h4>
          <p>Implementing AI-driven solutions, especially those requiring extensive computational resources like machine learning models or large language models (LLMs), can be expensive. To mitigate this, organizations can:</p>
          <ul>
              <li>Optimize AI usage by employing batch processing where real-time validation is unnecessary, reducing API calls and computational costs.</li>
              <li>Use pre-trained models (like those from GPT-4 or spaCy) and fine-tune them on smaller datasets, which can drastically reduce the cost compared to building custom models from scratch.</li>
              <li>Leverage cloud services with flexible pricing models that offer scalable infrastructure, paying only for the resources used.</li>
          </ul>
      
          <h4>Complexity</h4>
          <p>AI tools can be complex to implement, often requiring specialized expertise. Organizations can address this by:</p>
          <ul>
              <li>Incorporating AutoML platforms that automate the process of building and training machine learning models, allowing non-experts to create AI-driven systems with minimal expertise.</li>
              <li>Partnering with AI service providers or using turnkey solutions that offer ready-to-use AI systems with built-in data validation functionalities.</li>
          </ul>
      
          <h4>Latency</h4>
          <p>Real-time AI validation, especially using sophisticated models like GPT-4, can introduce latency that impacts user experience. Possible solutions include:</p>
          <ul>
              <li>Utilizing edge computing where AI models run on local devices or servers closer to the data source, reducing response times and improving performance.</li>
              <li>Implementing asynchronous processing where data is validated in parallel without holding up the main process.</li>
          </ul>
      
          <h4>Bias and Errors</h4>
          <p>AI models, especially those trained on historical data, can inherit biases, potentially leading to inaccurate or unfair outcomes. To mitigate this:</p>
          <ul>
              <li>Incorporate bias detection and mitigation tools that regularly audit models for biased behavior.</li>
              <li>Train models on diverse and representative datasets to ensure fairness across various user groups.</li>
              <li>Ensure human-in-the-loop validation, where AI decisions are reviewed by human experts to correct errors or biases.</li>
          </ul>
      
          <h3>2. Deeper Dive into Specific Tools</h3>
          <p>While several AI tools can assist with data validation, understanding the strengths and limitations of each is crucial for optimal usage:</p>
      
          <h4>GPT-4</h4>
          <p>GPT-4 excels at tasks involving unstructured text, such as text correction, entity extraction, and anomaly detection. It’s ideal when the data is rich in natural language and requires semantic understanding. Use GPT-4 when:</p>
          <ul>
              <li>You need high flexibility in handling various types of text inputs.</li>
              <li>There's a requirement for more nuanced, context-aware validation (e.g., interpreting ambiguous user inputs or detecting nuanced errors).</li>
          </ul>
      
          <h4>LangChain</h4>
          <p>LangChain shines in applications where multiple AI-driven tasks need to be chained together. It's particularly useful when combining the output of large language models with structured data processing tasks. Use LangChain when:</p>
          <ul>
              <li>You are building workflows that integrate multiple models, such as combining GPT-4's text analysis with additional tools for formatting or validation.</li>
              <li>Your application requires advanced prompt management and retry logic, ensuring consistent, accurate results from LLMs.</li>
          </ul>
      
          <h4>Playwright</h4>
          <p>Playwright is best suited for automating browser-based interactions, making it ideal for web scraping and validating user-generated content in real-time. Use Playwright when:</p>
          <ul>
              <li>You need to automate validation tasks that require interaction with dynamic web content or data submitted via forms.</li>
              <li>Real-time testing of user interactions or validation of live web data is required.</li>
          </ul>
      
          <h3>3. Ethical Considerations</h3>
          <p>As AI becomes more prevalent in data validation, ethical considerations should not be overlooked. Ensuring fairness, transparency, and accountability in AI-driven decisions is critical for maintaining trust and credibility:</p>
      
          <h4>Fairness</h4>
          <p>AI models can inadvertently perpetuate biases from training data. For instance, if biased datasets are used to train validation systems, it could lead to unfair outcomes for certain groups. To ensure fairness:</p>
          <ul>
              <li>Regularly audit AI models for bias and adjust the datasets or algorithms where necessary.</li>
              <li>Implement fairness checks to prevent models from discriminating against specific demographics.</li>
          </ul>
      
          <h4>Transparency</h4>
          <p>AI models, particularly deep learning models, are often seen as "black boxes," meaning their decision-making processes are not easily interpretable. To improve transparency:</p>
          <ul>
              <li>Use explainable AI (XAI) techniques to make model decisions more understandable to stakeholders.</li>
              <li>Provide users and analysts with clear explanations of how and why validation decisions are made by AI systems.</li>
          </ul>
      
          <h4>Accountability</h4>
          <p>As AI systems take over more decision-making processes, organizations must ensure that accountability is maintained. This means:</p>
          <ul>
              <li>Ensuring that there are clear human oversight mechanisms in place when AI is involved in critical validation tasks.</li>
              <li>Maintaining clear audit trails for how AI decisions are made, allowing for easy review and correction if necessary.</li>
          </ul>
      
          <p>By addressing these challenges and ethical considerations, organizations can better leverage AI for data validation in ways that are not only efficient but also fair, transparent, and trustworthy.</p>
      </section>
      
      <hr>
      

        <section id="future" class="animate__animated animate__fadeInUp">
            <h2>Future Technological Changes</h2>
            <ul>
                <li><strong>Multimodal Models</strong>: Integration of text, images, and other data types for comprehensive validation.</li>
                <li><strong>Edge AI</strong>: Running AI models on local devices to reduce latency and improve privacy.</li>
                <li><strong>AutoML</strong>: Automated machine learning tools to simplify model training and deployment.</li>
                <li><strong>Ethical AI</strong>: Emphasis on fairness, transparency, and accountability in AI systems.</li>
            </ul>
        </section>

        <hr>

        <section id="conclusion" class="animate__animated animate__fadeInUp">
            <h2>Conclusion</h2>
            <p>
              Implementing AI in data validation significantly enhances the integrity and reliability of datasets on user-generated content platforms. With the vast amounts of data being produced and uploaded daily, ensuring that this data is accurate, consistent, and relevant has become increasingly challenging. Traditional validation methods often rely on manual processes or static rules-based systems, which can be time-consuming, prone to errors, and difficult to scale. By introducing AI into the data validation pipeline, organizations can automate and streamline these processes, ultimately improving both efficiency and the quality of data.
            </p>
            <p>
              AI-powered tools, such as OpenAI’s GPT-4, spaCy, TensorFlow, LangChain, and Playwright, are transforming the way data validation is conducted. For instance, OpenAI’s GPT-4 can be used to comprehend and process complex, unstructured text, flagging anomalies or inconsistencies based on semantic understanding. This is particularly valuable in platforms where user-generated content can vary greatly in style, language, and structure. The natural language processing capabilities of spaCy enable the identification of linguistic patterns and can help in flagging inappropriate or irrelevant content. TensorFlow, a powerful machine learning framework, can be utilized to build models that predict potential data entry errors, detect fraudulent activities, or classify data into meaningful categories, all while learning from historical data to improve its accuracy over time.
            </p>
            <p>
              LangChain, in particular, enables seamless integration of large language models (LLMs) into data validation workflows by providing a framework for chaining multiple AI-based operations together. It can, for example, combine GPT-4’s semantic analysis capabilities with the structured outputs of other AI tools to ensure comprehensive validation of data. Playwright, an automation framework for browser testing, allows for dynamic interaction with web platforms, making it possible to validate content or test user flows in real time by simulating how real users might interact with data or content submission forms. This combination of tools offers a multi-faceted approach to data validation, addressing various aspects of content integrity, from linguistic checks to behavior simulations.
            </p>
            <p>
              Automating these complex validation tasks has profound implications for data quality management. It ensures that datasets are consistently monitored for accuracy, completeness, and compliance with platform-specific guidelines or regulatory requirements. Automated validation also reduces the reliance on human intervention, allowing data analysts to focus on higher-level tasks, such as refining algorithms or interpreting validation results. Moreover, it enhances the overall user experience by quickly identifying and rectifying issues without causing delays or interruptions in the user-generated content submission process.
            </p>
            <p>
              However, while the benefits of integrating AI into data validation pipelines are substantial, challenges remain. One of the key concerns is ensuring the transparency and interpretability of AI-driven decisions, particularly when dealing with complex models such as GPT-4. Users and data analysts must trust that AI is making fair and accurate decisions, which may require additional efforts in terms of model explainability and bias mitigation. Additionally, as platforms grow in scale and diversity, AI models must be continuously updated and trained on new data to avoid obsolescence. Another challenge lies in balancing automation with the nuances of human judgment, especially in subjective areas like content moderation or ethical considerations.
            </p>
            <p>    
              Despite these challenges, the advantages of AI-driven data validation far outweigh the potential drawbacks. With the ability to handle vast amounts of data in real time, AI empowers platforms to maintain high standards of data quality without sacrificing user experience or operational efficiency. As user-generated content platforms continue to grow, the integration of AI into data pipelines will not only be a valuable investment but also a necessity to stay competitive in the ever-evolving digital landscape.
            </p>
            <p>    
              In conclusion, leveraging AI tools like GPT-4, spaCy, TensorFlow, LangChain, and Playwright offers a robust solution to the complexities of modern data validation. By automating repetitive and intricate tasks, improving the accuracy of data processing, and ensuring compliance with platform standards, AI enables organizations to maintain reliable, high-quality datasets at scale. While there are challenges to be addressed, the strategic implementation of AI in data validation represents a forward-thinking approach that will play a critical role in shaping the future of user-generated content platforms.
            </p>
        </section>

        <hr>

        <section id="resources" class="animate__animated animate__fadeInUp">
            <h2>Additional Resources</h2>
            <ul>
                <li><strong>OpenAI API Documentation</strong>: <a href="https://beta.openai.com/docs/">https://beta.openai.com/docs/</a></li>
                <li><strong>spaCy Documentation</strong>: <a href="https://spacy.io/usage">https://spacy.io/usage</a></li>
                <li><strong>TensorFlow Tutorials</strong>: <a href="https://www.tensorflow.org/tutorials">https://www.tensorflow.org/tutorials</a></li>
                <li><strong>LangChain Documentation</strong>: <a href="https://langchain.readthedocs.io/en/latest/">https://langchain.readthedocs.io/en/latest/</a></li>
                <li><strong>Playwright for Python</strong>: <a href="https://playwright.dev/python/docs/intro">https://playwright.dev/python/docs/intro</a></li>
            </ul>
            <p><strong>Note</strong>: Replace <code>'YOUR_OPENAI_API_KEY'</code> with your actual OpenAI API key in the code examples. Always ensure compliance with the terms of service for any API or tool you use.</p>
        </section>
    </main>

    <footer class="text-center text-white bg-dark py-4">
        <p>&copy; 2024 Data Validation Guide by Liam Scott</p>
    </footer>

    <button onclick="topFunction()" id="backToTop" title="Go to top">↑</button>

    <script
      src="https://code.jquery.com/jquery-3.5.1.min.js"
    ></script>
    <script
      src="https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js"
    ></script>
    <script
      src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-python.min.js"
    ></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/components/prism-json.min.js"
    ></script>
    <script>
        $('a.nav-link').on('click', function(event) {
          if (this.hash !== '') {
            event.preventDefault();
            var hash = this.hash;
            $('html, body').animate(
              {
                scrollTop: $(hash).offset().top - 70,
              },
              800
            );
          }
        });

        var backToTopButton = document.getElementById('backToTop');

        window.onscroll = function() {
          scrollFunction();
        };

        function scrollFunction() {
          if (
            document.body.scrollTop > 500 ||
            document.documentElement.scrollTop > 500
          ) {
            backToTopButton.style.display = 'block';
          } else {
            backToTopButton.style.display = 'none';
          }
        }

        function topFunction() {
          $('html, body').animate({ scrollTop: 0 }, 800);
        }
    </script>
</body>
</html>
